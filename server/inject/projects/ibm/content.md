# Placement at IBM

My placement at IBM started as a DevOps role, being part of the Systems business unit in the z Release Engineering Team where we dealt with development builds, general infrastructure and client production servers. Initially, I was given the standalone project of centralising logging from a set of Liberty servers known as Colony, receiving strict requirements such as the tech stack and the cloud provider but with enough freedom to innovate. Therefore, my client was my team, as this monitoring solution was something they were seeking to implement since it would greatly benefit their daily operations and their awareness of the general health of their infrastructure.

It all started with me doing education on various pieces of technology; IBM has a great learning platform where I could choose any topic and become somewhat versed in a relatively short amount of time. I began by learning Elasticsearch, the tool that allowed me to index and organise the logs generated by Liberty and perform complex queries such as How many times last month did the number of error messages become greater than the daily average? Or How has the ratio of status codes changed for those of last week? Then I continued by learning about Logstash, which you can think of as a parser or a filter that enabled me to add, remove and process the different kinds of logs these servers produce, as their format slightly changes depending on its source. Finally, I learnt about Kibana, the UI for creating and visualising log data in user-created dashboards. This was a fun process since I could see in real-time the various logs coming into the Elasticsearch cluster after being parsed by Logstash; Kibana would become the tool the team would use to achieve the previously mentioned goal of increasing their awareness of their infrastructure.

My initial prototype consisted of an Elasticsearch 3-node cluster automated using Ansible hosted on our private cloud here in Hursley, and the objective was not to make it the final solution but rather use it as a proof of work and think of ways we could improve its functionalities. It featured a Kibana dashboard where you could see the different logs that a mock Liberty server generated, this server being something I set up myself as a test before running this on actual production machines.

During the year, I delivered multiple playback presentations where I would summarise my work to the rest of the team and, sometimes, to other teams. This would prove to be critical in developing the final implementation as it allowed me to engage in continuous feedback loops with the final users of my architecture. It was also helpful for me to develop presentation skills, which are very important in this industry since knowledge sharing is a challenging but necessary step in the success of any project.

After the summer, my manager introduced me to the idea of using Kubernetes. The approach to date had been to deploy Elasticsearch, Logstash and Kibana as separated binaries on different environments bootstrapped by Ansible, but the reliability of those environments and the replicability of the architecture was a core issue. Therefore, it only made sense to enter the world of microservices and containerisation using technologies like Docker and Kubernetes. So I started researching and learning about the inner workings of Docker containers, the creation of Dockerfiles and the behaviour of different tools like docker-compose.

After this education period concluded, the research indicated that Kubernetes was the best solution for our use case; hence I started to design the system where the Elastic Stack would be run. My idea was to save future development efforts and fully automate the formation of generic Kubernetes clusters that we could modify to accommodate our use case of centralised logs. Meanwhile, other teams could use it for their particular use cases without being restricted to our requirements. This was mainly achieved through creating Ansible playbooks and Terraform manifests, which would define the cloud resources as Infrastructure as Code and set up the cluster with reusable scripts.

To date, this education has been the most crucial technology I've learnt throughout my year. It has made me more aware of the current trends in the industry to the point that I am now hosting a Kubernetes cluster myself as a hobby where I can learn to deploy applications and play around with its features. The ecosystem of microservices and cloud computing is something I would want to see being taught at universities for its importance and the critical role it plays today in the landscape of enterprise applications.

Sometime around February, I was approached by a colleague to work on a quick Python script that, as a result of the UK government suspending some of our export licenses to Russia, would analyse which accounts were on high-risk systems so that they could be taken down. This was an exciting experience for me as I felt like interns play a role in the daily functions of IBM as a whole, which is always good to see.

It was around this time, too, that I started collaborating each Friday with an IBM Research team to develop a Natural Language Processing model to detect media bias in news articles. This was possible thanks to my manager, as I had told him previously of my interests in AI and my willingness put effort into such activity. The leading technologies I used were Python and Docker, with a particular focus on machine learning libraries like PyTorch, TensorFlow and Pandas. My main contributions involved the preprocessing of an internal dataset populated by scraped news content websites, the fine-tuning of a BERT language model and the consequent analysis of its predictions and evaluation potential. This would end up being a recurrent source of education throughout the last half of my year that would culminate in me earning the Machine Learning Specialist (Professional) badge, which is a 70+ hr course taught by IBM on the fundamentals of exploratory data analysis, supervised & unsupervised machine learning models, recurrent & convolutional neural networks, autoencoders, ethics in AI and AI governance.

The Elastic Stack monitoring solution improved my data science skills towards data visualisation and large index management. Some of the visualisations I created consisted of word clouds for error and log message types, bar and line charts for metrics such as the number of indexed logs and response times, pie charts for the ratio of logs coming from different servers and a Sankey graph which I'm most proud of since it's a pretty helpful visualisation that you wouldn't be able to aggregate yourself by just looking at the logs. It conveys the relationships between the server names and the public IPs of connecting clients, enabling you to understand the activity on your servers and the client's behaviour, as you can see the other servers they are linking to. The dashboard is a self-contained solution that I'm happy with in the state I've left it at, although the team can always modify it to suit their future needs.

Another interesting concept I've learnt about throughout this year has been the importance of secret and credential management. Before coming to IBM, I wasn't aware of the exact techniques used to keep sensitive data protected yet accessible to applications; my team had a self-hosted installation of Vault, an open-source solution for encrypting and sealing secrets. This is a tool that I've used constantly; from the Ansible playbooks to the Jenkins pipelines, Vault has proven to be our primary source of truth for credentials.

After many playback presentations, feedback sessions and user testing activities, the solution was completed in July of 2022, consisting of a fully automated 4-node Kubernetes cluster, bootstrapped by a set of Terraform manifests and setup by Ansible playbooks abstracted into reusable roles. The spawned infrastructure ended up hosting: an Elasticsearch daemon set, a Logstash deployment, a Kibana deployment, an OpenStack Cloud Controller Manager, a Cinder CSI storage class, an Octavia Ingress Controller and many other smaller applications such as Container Network Interface. In terms of metrics, the cluster had indexed around 5 million logs in a month and a half from 3 different production Liberty servers although the numbers will increase as more servers are added to the monitoring pool. The automation of this project has had a great emphasis, as I have delivered: a set of Ansible playbooks for the maintenance of the Kubernetes cluster, a collection of Jenkinsfiles written in Groovy for the automatic setup and TLS certificate update of Liberty servers and Ansible playbooks to automate the bootstrap of a distributed storage solution named Ceph.

We organised high levels of engagement and social activities with the other interns, from weekly virtual games to physical events, such as when we rented a boat and hosted an IBM interns' boat party sailing on the river Thames in London. We also had the opportunity to attend multiple barbeque parties in Hursley, where Arvind Krishna, the CEO of the whole of IBM, attended himself with many other high-level executives.

 I can only be grateful and humbled by this experience, which will surely stick with me for the rest of my professional life as the first stepping-stone of my career.
